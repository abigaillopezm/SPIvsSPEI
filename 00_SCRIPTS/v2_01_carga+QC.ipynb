{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a944b106",
   "metadata": {},
   "source": [
    "*upd. 14-feb-2026*\n",
    "Se seleccionaron tres estaciones para hacer una evaluación preliminar de los índices SPEI y SPI. Estas están distribuidas en el estado de Jalisco, específicamente, en la zona costa (Puerto Vallarta), centro (Guadalajara) y altos (Lagos Moreno). En esta primera parte del script, se hace la limpieza y análisis exploratorio de los CSV descargados del SMN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a6f8c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#se importan las librerias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1776db5b",
   "metadata": {},
   "source": [
    "##### Limpieza de los datos\n",
    "1. Se usa `skiprows`y `usecols`para leer desde el principio solo la información necesaria\n",
    "2. Se convierten los valores con la etiqueta NULOS a *NaN*\n",
    "3. Se elimina la fila que contiene strings con las unidades para que no estorben en futuros cálculos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bd0b4a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carga_smn(id_estac):\n",
    "    ruta = f\"../01_DATA/raw/dia{id_estac}.txt\"\n",
    "\n",
    "    #Leer los archivos evitando los metadatos para que no estorben al hacer lo cálculos\n",
    "    df = pd.read_csv(ruta, sep='\\\\s+', na_values=['NULO'], usecols = ('FECHA', 'PRECIP','TMAX', 'TMIN'),skiprows=19)\n",
    "    \n",
    "    #Se elimina la fila de las unidades por el mismo motivo anterior\n",
    "    df = df.drop(index=0).reset_index(drop=True)\n",
    "        \n",
    "    #poner columnas en el formato adecuado de variable\n",
    "    df['FECHA'] = pd.to_datetime(df['FECHA'])\n",
    "    cols_df = ['PRECIP', 'TMAX', 'TMIN']\n",
    "    for i in cols_df:\n",
    "        df[i] = pd.to_numeric(df[i], errors = 'coerce')\n",
    "    return df\n",
    "\n",
    "# ---- Para agilizar. se usa un bucle\n",
    "estacs = [\"14339\",\"14067\", \"14024\", #costa\n",
    "          \"14066\", \"14065\", \"14169\", #centro\n",
    "          \"14084\", \"14083\"] #altos\n",
    "\n",
    "# se hace un diccionario para que no se sobreescriban los archivos\n",
    "estacs_leidas = {}\n",
    "\n",
    "for id_estac in estacs:\n",
    "    resultado = carga_smn(id_estac)\n",
    "    estacs_leidas[id_estac] = resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ba22ea39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       FECHA  PRECIP  TMAX  TMIN\n",
      "0 1980-06-15     NaN   NaN   NaN\n",
      "1 1980-06-16     0.0  31.0  12.0\n",
      "2 1980-06-17     0.0  32.0  18.0\n",
      "3 1980-06-18     0.0  34.0  18.0\n",
      "4 1980-06-19     0.0  32.0  18.0\n",
      "           FECHA  PRECIP  TMAX  TMIN\n",
      "16180 2026-01-30     NaN   NaN   NaN\n",
      "16181 2026-02-01     9.2   NaN   NaN\n",
      "16182 2026-02-02     NaN   NaN   NaN\n",
      "16183 2026-02-04     0.0   NaN   NaN\n",
      "16184 2026-02-05     NaN   NaN   NaN\n"
     ]
    }
   ],
   "source": [
    "print(estacs_leidas['14339'].head())\n",
    "print(estacs_leidas['14339'].tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdf151e",
   "metadata": {},
   "source": [
    "##### Control de calidad de los datos\n",
    "\n",
    "**Límites físicos / outliers**\n",
    "* Consistencia térmica: en caso de que Tmax < Tmin, se eliminarán ambos valores\n",
    "* Validación de mínimo: Prcp. no puede ser negativa :. si es negativo se elimina el valor\n",
    "\n",
    "**Estandarización de las fechas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6d98e0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando QC para Estación 14339...\n",
      "QC: 0 errores térmicos corregidos. Total: 16185 registros.\n",
      "Procesando QC para Estación 14067...\n",
      "QC: 0 errores térmicos corregidos. Total: 23706 registros.\n",
      "Procesando QC para Estación 14024...\n",
      "QC: 0 errores térmicos corregidos. Total: 23453 registros.\n",
      "Procesando QC para Estación 14066...\n",
      "QC: 0 errores térmicos corregidos. Total: 31265 registros.\n",
      "Procesando QC para Estación 14065...\n",
      "QC: 0 errores térmicos corregidos. Total: 23910 registros.\n",
      "Procesando QC para Estación 14169...\n",
      "QC: 0 errores térmicos corregidos. Total: 26144 registros.\n",
      "Procesando QC para Estación 14084...\n",
      "QC: 0 errores térmicos corregidos. Total: 29627 registros.\n",
      "Procesando QC para Estación 14083...\n",
      "QC: 0 errores térmicos corregidos. Total: 23423 registros.\n"
     ]
    }
   ],
   "source": [
    "def qc_logico(df):   \n",
    "    # 1. Consistencia Térmica: La Máxima no puede ser menor a la Mínima\n",
    "    # Identificamos errores de captura/digitalización\n",
    "    mask_temp_error = df['TMIN'] > df['TMAX']\n",
    "    \n",
    "    # Si TMIN > TMAX, invalidamos ambos registros asignando NaN\n",
    "    df.loc[mask_temp_error, ['TMAX', 'TMIN']] = np.nan\n",
    "    \n",
    "    # 2. Límites Físicos: Valores negativos imposibles\n",
    "    # La precipitación >= 0\n",
    "    df.loc[df['PRECIP'] < 0, 'PRECIP'] = np.nan\n",
    "    \n",
    "    # --- INDEXACIÓN CON FECHAS ---\n",
    "    # 3. Asegurar que FECHA sea tipo datetime\n",
    "    df['FECHA'] = pd.to_datetime(df['FECHA'], errors='coerce')\n",
    "    \n",
    "    # 4. Eliminar duplicados de fecha antes de indexar\n",
    "    df = df.drop_duplicates(subset=['FECHA'])\n",
    "    \n",
    "    # 5. Establecer FECHA como índice\n",
    "    df = df.set_index('FECHA')\n",
    "    \n",
    "    # 6. Ordenar cronológicamente\n",
    "    df = df.sort_index()\n",
    "    \n",
    "    # Reporte de limpieza para seguimiento\n",
    "    # Usamos len(mask_temp_error) o el ID si lo tuviéramos para identificar\n",
    "    print(f\"QC: {mask_temp_error.sum()} errores térmicos corregidos. Total: {len(df)} registros.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- BUCLE PARA PROCESAR EL DICCIONARIO ---\n",
    "\n",
    "# Creamos un nuevo diccionario para los datos limpios\n",
    "estaciones_qc = {}\n",
    "\n",
    "for id_estac, df_raw in estacs_leidas.items():\n",
    "    print(f\"Procesando QC para Estación {id_estac}...\")\n",
    "    # Aplicamos la función y guardamos el resultado\n",
    "    estaciones_qc[id_estac] = qc_logico(df_raw)\n",
    "\n",
    "# Ahora puedes acceder a cualquier estación limpia así:\n",
    "# estaciones_qc['14002'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4c488c",
   "metadata": {},
   "source": [
    "##### Integridad de la serie\n",
    "**Faltantes**\n",
    "* Calcular cantidad de datos faltantes\n",
    "* se considerarán las estaciones que contienen más del 90% de los datos\n",
    "* Calidad Mensual: Que no haya meses con más de 5 días faltantes (estos se vuelven NaN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "18541fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estación 14339: Calendario normalizado.\n",
      "Estación 14067: Calendario normalizado.\n",
      "Estación 14024: Calendario normalizado.\n",
      "Estación 14066: Calendario normalizado.\n",
      "Estación 14065: Calendario normalizado.\n",
      "Estación 14169: Calendario normalizado.\n",
      "Estación 14084: Calendario normalizado.\n",
      "Estación 14083: Calendario normalizado.\n",
      "\n",
      "--- NORMALIZACIÓN GLOBAL FINALIZADA ---\n",
      "Rango de estudio Jalisco: 1882-04-01 a 2026-02-05\n",
      "Longitud de la serie: 52541 días.\n"
     ]
    }
   ],
   "source": [
    "def integridad_temporal_masiva(diccionario_estaciones):\n",
    "    # 1. Validar que el diccionario no esté vacío\n",
    "    if not diccionario_estaciones:\n",
    "        print(\"El diccionario de estaciones está vacío.\")\n",
    "        return {}\n",
    "\n",
    "    # 2. Identificar la fecha mínima y máxima global de TODO el set de Jalisco\n",
    "    # Recorremos todos los DataFrames para encontrar los extremos temporales\n",
    "    fecha_min_global = min([df.index.min() for df in diccionario_estaciones.values()])\n",
    "    fecha_max_global = max([df.index.max() for df in diccionario_estaciones.values()])\n",
    "    \n",
    "    # 3. Crear el calendario maestro diario (sin saltos)\n",
    "    calendario_maestro = pd.date_range(start=fecha_min_global, end=fecha_max_global, freq='D')\n",
    "    \n",
    "    estaciones_normalizadas = {}\n",
    "    \n",
    "    # 4. Bucle para reindexar cada estación con el calendario maestro\n",
    "    for id_est, df in diccionario_estaciones.items():\n",
    "        # .reindex() es la clave: inserta filas con NaN en las fechas faltantes\n",
    "        df_norm = df.reindex(calendario_maestro)\n",
    "        df_norm.index.name = 'FECHA'\n",
    "        \n",
    "        # Marcamos qué días son \"rellenados\" (útil para auditoría de datos)\n",
    "        # Si PRECIP es NaN, asumimos que es un dato faltante insertado ahora\n",
    "        df_norm['MISSING_DAY'] = df_norm['PRECIP'].isna().astype(int)\n",
    "        \n",
    "        estaciones_normalizadas[id_est] = df_norm\n",
    "        print(f\"Estación {id_est}: Calendario normalizado.\")\n",
    "    \n",
    "    print(f\"\\n--- NORMALIZACIÓN GLOBAL FINALIZADA ---\")\n",
    "    print(f\"Rango de estudio Jalisco: {fecha_min_global.date()} a {fecha_max_global.date()}\")\n",
    "    print(f\"Longitud de la serie: {len(calendario_maestro)} días.\")\n",
    "    \n",
    "    return estaciones_normalizadas\n",
    "\n",
    "# --- EJECUCIÓN ---\n",
    "\n",
    "# Usamos el diccionario 'estaciones_qc' que generamos en el Bloque 2\n",
    "estaciones_listas = integridad_temporal_masiva(estaciones_qc)\n",
    "\n",
    "# Ejemplo de verificación:\n",
    "#print(estaciones_listas['14002'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78254eae",
   "metadata": {},
   "source": [
    "##### Guardar lo anterior como CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5627c48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estación 14339: Guardada con éxito (10958 días).\n",
      "Estación 14067: Guardada con éxito (10958 días).\n",
      "Estación 14024: Guardada con éxito (10958 días).\n",
      "Estación 14066: Guardada con éxito (10958 días).\n",
      "Estación 14065: Guardada con éxito (10958 días).\n",
      "Estación 14169: Guardada con éxito (10958 días).\n",
      "Estación 14084: Guardada con éxito (10958 días).\n",
      "Estación 14083: Guardada con éxito (10958 días).\n",
      "\n",
      "--- PROCESO FINALIZADO ---\n"
     ]
    }
   ],
   "source": [
    "def exportar_rango_especifico(diccionario_normalizado, carpeta_salida, fecha_inicio, fecha_fin):\n",
    "    if not os.path.exists(carpeta_salida):\n",
    "        os.makedirs(carpeta_salida)\n",
    "\n",
    "    for id_est, df in diccionario_normalizado.items():\n",
    "        # 1. Filtrar el DataFrame usando el índice (que es la FECHA)\n",
    "        # .loc es inclusivo en ambos extremos\n",
    "        try:\n",
    "            df_filtrado = df.loc[fecha_inicio : fecha_fin]\n",
    "            \n",
    "            if df_filtrado.empty:\n",
    "                print(f\"Estación {id_est}: Sin datos en el rango {fecha_inicio} a {fecha_fin}. Saltando...\")\n",
    "                continue\n",
    "\n",
    "            # 2. Guardar el archivo filtrado\n",
    "            nombre_archivo = f\"SMN_{id_est}_{fecha_inicio[:4]}_{fecha_fin[:4]}.csv\"\n",
    "            ruta_completa = os.path.join(carpeta_salida, nombre_archivo)\n",
    "            \n",
    "            df_filtrado.to_csv(ruta_completa, sep=',', encoding='utf-8', index=True)\n",
    "            print(f\"Estación {id_est}: Guardada con éxito ({len(df_filtrado)} días).\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error al filtrar la estación {id_est}: {e}\")\n",
    "\n",
    "    print(f\"\\n--- PROCESO FINALIZADO ---\")\n",
    "\n",
    "# --- EJECUCIÓN ---\n",
    "# Define tus fechas de interés (ejemplo: periodo de 30 años)\n",
    "inicio = '1991-01-01'\n",
    "fin = '2020-12-31'\n",
    "carpeta_proyectos = \"../01_DATA/SMN_limpio\"\n",
    "\n",
    "exportar_rango_especifico(estaciones_listas, carpeta_proyectos, inicio, fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2053b4b8",
   "metadata": {},
   "source": [
    "# Selección de estaciones\n",
    "Las estaciones que se trabajaran son las que cumplan los siguientes criterios\n",
    "a) tienen el 90% de registros (datos faltantes < 10%)\n",
    "b) no pueden tener cinco registro nulos seguidos \n",
    "\n",
    "**Se eligió como periodo de 1991 a 2020 (30 años**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dc33a5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando filtrado estricto de 8 estaciones...\n",
      "❌ SMN_14024_1991_2020.csv: Descartado por Racha >= 15 días NaN, Meses con 15 5 NaNs\n",
      "❌ SMN_14065_1991_2020.csv: Descartado por Racha >= 15 días NaN, Meses con 15 5 NaNs\n",
      "✅ SMN_14066_1991_2020.csv: APTO (Faltantes: 0.1%)\n",
      "❌ SMN_14067_1991_2020.csv: Descartado por Racha >= 15 días NaN, Meses con 15 5 NaNs\n",
      "❌ SMN_14083_1991_2020.csv: Descartado por Racha >= 15 días NaN, Meses con 15 5 NaNs\n",
      "❌ SMN_14084_1991_2020.csv: Descartado por Racha >= 15 días NaN, Meses con 15 5 NaNs\n",
      "❌ SMN_14169_1991_2020.csv: Descartado por Racha >= 15 días NaN, Meses con 15 5 NaNs\n",
      "✅ SMN_14339_1991_2020.csv: APTO (Faltantes: 0.1%)\n",
      "\n",
      "--- FILTRADO TERMINADO ---\n",
      "Estaciones finales en 'for_indices': 2\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURACIÓN DE RUTAS ---\n",
    "PATH_ENTRADA = \"../01_DATA/SMN_limpio/\"\n",
    "PATH_SALIDA = \"../01_DATA/for_indices\"\n",
    "FECHA_INICIO = \"1991-01-01\"\n",
    "FECHA_FIN = \"2020-12-31\"\n",
    "\n",
    "def tiene_rachas_nan(serie, max_consecutivos=5):\n",
    "    # Creamos una serie booleana (True donde es NaN)\n",
    "    es_nan = serie.isna()\n",
    "    # Identificamos bloques consecutivos de True\n",
    "    # (Comparamos el valor con el anterior para marcar cambios de bloque)\n",
    "    bloques = es_nan.ne(es_nan.shift()).cumsum()\n",
    "    # Contamos el tamaño de cada bloque de NaNs\n",
    "    tamaño_bloques = es_nan.groupby(bloques).transform('sum')\n",
    "    # Si algún bloque de NaNs es >= al límite, devolvemos True\n",
    "    return (tamaño_bloques >= max_consecutivos).any()\n",
    "\n",
    "def filtrar_estaciones_estricto():\n",
    "    archivos = [f for f in os.listdir(PATH_ENTRADA) if f.endswith('.csv')]\n",
    "    aptas = 0\n",
    "    \n",
    "    print(f\"Iniciando filtrado estricto de {len(archivos)} estaciones...\")\n",
    "\n",
    "    for archivo in archivos:\n",
    "        df = pd.read_csv(os.path.join(PATH_ENTRADA, archivo), \n",
    "                         index_col='FECHA', parse_dates=True)\n",
    "        \n",
    "        # 1. Recorte temporal\n",
    "        df_p = df.loc[FECHA_INICIO : FECHA_FIN]\n",
    "        \n",
    "        # 2. Validación de existencia de datos\n",
    "        if df_p.empty or len(df_p) < 1000: # Filtro básico de seguridad\n",
    "            continue\n",
    "\n",
    "        # --- CRITERIOS DE CALIDAD ---\n",
    "        \n",
    "        # A. Máximo 10% de faltantes totales\n",
    "        pct_nan = (df_p['PRECIP'].isna().sum() / len(df_p)) * 100\n",
    "        \n",
    "        # B. No más de 5 días SEGUIDOS con NaN\n",
    "        racha_nan = tiene_rachas_nan(df_p['PRECIP'], max_consecutivos=15)\n",
    "        \n",
    "        # C. No más de 5 días faltantes en UN MES (alternos o no)\n",
    "        # Esto asegura que cada mes sea representativo para el SPI\n",
    "        meses_invalidos = (df_p['PRECIP'].isna().resample('MS').sum() > 15).any()\n",
    "\n",
    "        # --- DECISIÓN FINAL ---\n",
    "        if pct_nan <= 20 and not racha_nan and not meses_invalidos:\n",
    "            ruta_destino = os.path.join(PATH_SALIDA, archivo)\n",
    "            df_p.to_csv(ruta_destino)\n",
    "            aptas += 1\n",
    "            print(f\"✅ {archivo}: APTO (Faltantes: {pct_nan:.1f}%)\")\n",
    "        else:\n",
    "            razon = []\n",
    "            if pct_nan > 20: razon.append(f\"Total NaNs > 20% ({pct_nan:.1f}%)\")\n",
    "            if racha_nan: razon.append(\"Racha >= 15 días NaN\")\n",
    "            if meses_invalidos: razon.append(\"Meses con 15 5 NaNs\")\n",
    "            print(f\"❌ {archivo}: Descartado por {', '.join(razon)}\")\n",
    "\n",
    "    print(f\"\\n--- FILTRADO TERMINADO ---\")\n",
    "    print(f\"Estaciones finales en 'for_indices': {aptas}\")\n",
    "\n",
    "filtrar_estaciones_estricto()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
